{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Tutorial on RNN - 3\n",
    "#### Notebook Author: Nirupam Purushothama\n",
    "\n",
    "**About:** Building a basic RNN using TensorFlow (barebones). Created using TensorFlow RNN functions\n",
    "\n",
    "Static unrolling through time - But input also provided as a batch input\n",
    "\n",
    "**Reference Book:** Hands-on Machine Learning with Scikit-Learn and TensorFlow - Aurelien Geron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-338ea05f8d13>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3 # Number of inputs per mini-batch\n",
    "n_steps = 2 # Number of time steps\n",
    "n_neurons = 5 # Number of neurons\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "# tf.transpose\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm=[1,0,2]))\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)\n",
    "\n",
    "# Gets the output into [None, n_steps, n_inputs] shape\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm=[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch: instance 0, instance 1, instance 2, instance 3\n",
    "X_batch = np.array([\n",
    "    [[0,1,2],[9,8,7]],\n",
    "    [[3,4,5],[0,0,0]],\n",
    "    [[6,7,8],[6,5,4]],\n",
    "    [[9,0,1],[3,2,1]],    \n",
    "])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 3)\n",
      "(2, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Understanding the transpose function. perm - The type of permutation you would require.\n",
    "# Below print is self-explanatory\n",
    "\n",
    "z = tf.transpose(X_batch, perm=[1,0,2])\n",
    "print(X_batch.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.89812666  0.5646168  -0.28691128  0.84830683  0.07136286]\n",
      "  [ 0.98208904  0.99992794 -0.99999934  0.98900819 -0.9992702 ]]\n",
      "\n",
      " [[ 0.99428833  0.98352897 -0.99286908  0.99106401 -0.72675556]\n",
      "  [-0.89091104  0.53958696 -0.21682814 -0.49988967 -0.79737538]]\n",
      "\n",
      " [[ 0.99969435  0.99950439 -0.99995381  0.99950927 -0.95751303]\n",
      "  [-0.05265796  0.99823546 -0.99989027  0.77930015 -0.99560475]]\n",
      "\n",
      " [[-0.99970752  0.99763662 -0.9958359  -0.34800643 -0.99767113]\n",
      "  [-0.70426148  0.47066903 -0.98827076  0.62712705 -0.78638566]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But this still creates one cell per step (internally) and the graph constructed will be huge and will generate out of memory errors. Hence we use Dynamic RNNs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
